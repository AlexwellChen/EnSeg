{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22598179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitFeatureExtractor, BeitForSemanticSegmentation, DetrFeatureExtractor, DetrForSegmentation\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# load ADE20k image\n",
    "# ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n",
    "train_img = \"../../ADEChallengeData2016/images/training/\"\n",
    "def get_img_path(path):\n",
    "    return [f for f in os.listdir(path)]\n",
    "\n",
    "# train_img_list = get_img_path(train_img)\n",
    "# input_image = Image.open(train_img + train_img_list[3])\n",
    "input_image = Image.open(\"deeplab1.png\")\n",
    "input_image = input_image.convert(\"RGB\")\n",
    "input_image = input_image.resize((640, 640))\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\n",
    "# model = BeitForSemanticSegmentation.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\n",
    "\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
    "model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
    "\n",
    "# feature_extractor.to(\"mps\")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "inputs = feature_extractor(images=input_image, return_tensors=\"pt\").to(\"cpu\")\n",
    "# outputs = model(**inputs)\n",
    "# # logits are of shape (batch_size, num_labels, height/4, width/4)\n",
    "# logits = outputs.logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61233148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-17.8258,  -1.0810,  -9.4990,  ..., -17.9978, -17.5670,  12.4708],\n",
       "         [-16.8797,  -1.3207,  -8.7598,  ..., -17.1013, -17.3081,  11.9791],\n",
       "         [-11.5861,   0.4764,   0.5151,  ..., -12.1454, -11.9308,   7.5341],\n",
       "         ...,\n",
       "         [-18.0635,  -1.1683,  -8.5681,  ..., -17.7080, -18.0266,  12.5173],\n",
       "         [-17.8515,  -2.8433,  -9.2421,  ..., -18.0970, -18.0145,  11.5757],\n",
       "         [-18.9103,  -4.4347, -10.9140,  ..., -19.1155, -18.8106,  12.0398]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9da64c2fa4a110d6089bfe4724cddcff253a9e81417e9b09016aa1a3b411001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
